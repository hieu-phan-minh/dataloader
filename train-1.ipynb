{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=1, size=(1, 256, 512)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_shifted = image_tensor\n",
    "    image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    def forward(self,x):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class ThinningNet(Module):\n",
    "\tdef __init__(self):       \n",
    "\t\tsuper(ThinningNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=64,kernel_size=9,padding=1)       \n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)       \n",
    "        self.conv3 = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1) \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = torch.relu(x)\n",
    "                  \n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir ='./dataset/'\n",
    "\n",
    "trainTransforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "trainDataset = ImageFolder(os.path.join(data_dir,'train'), transform = trainTransforms)\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "img_names = []\n",
    "\n",
    "path = os.path.join(data_dir,'train/')\n",
    "\n",
    "print('Path:',path)\n",
    "\n",
    "for folder,subfolders,filenames in os.walk(path):\n",
    "    for img in filenames:\n",
    "        img_names.append(folder+\"/\"+img)\n",
    "        #print(img_names)\n",
    "\n",
    "pic = 10\n",
    "\n",
    "img_test = Image.open(img_names[pic])\n",
    "print(img_names[pic])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "im = transform(img_test)\n",
    "\n",
    "print('Shape tensor:', im.shape)\n",
    "\n",
    "im_width =  im.shape[2]\n",
    "\n",
    "im_input  = im[:, :, :im_width // 2]\n",
    "im_output = im[:, :, im_width // 2:]\n",
    "\n",
    "show_tensor_images(im,)\n",
    "\n",
    "show_tensor_images(im_input,1,(1, 256, 256))\n",
    "show_tensor_images(im_output,1,(1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
